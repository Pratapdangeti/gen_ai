{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community\n"
      ],
      "metadata": {
        "id": "KGDr-aHaJ42I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d2cd22-1fde-4fc6-e36a-3aeb29c0b4de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.68)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.4)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.7.9)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain_community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc0qKhuQJ0WQ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"API_Key\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-shot prompting with direct LLM Call\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=openai.api_key)\n",
        "\n",
        "_system_message = \"\"\"\n",
        "You are a tourism expert. You should be respectful to the users and\n",
        "deliver responses in a friendly manner. In case if you don't have confidence,\n",
        "just say don't know. Output your responses in JSON format.\n",
        "\"\"\"\n",
        "\n",
        "_user_message = \"\"\" I am planning to travel Western side of Kashmir during\n",
        "the Summer season by road. Provide me the list of 4 visiting places in Kashmir\n",
        "in no more than 30 words.\n",
        "\"\"\"\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    temperature=0.3,\n",
        "    top_p=0.4,\n",
        "    messages=[\n",
        "        {\"role\":\"system\",\"content\":_system_message},\n",
        "        {\"role\":\"user\",\"content\":_user_message}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNE6EElnfg-z",
        "outputId": "81b65626-caa1-44a7-d435-c3dcd2c84205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"places\": [\n",
            "    \"Gulmarg – Famous for its lush meadows and cable car rides.\",\n",
            "    \"Pahalgam – Known for scenic valleys and Lidder River.\",\n",
            "    \"Sonamarg – Renowned for golden meadows and trekking.\",\n",
            "    \"Yusmarg – Serene landscapes and pine forests.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-shot prompting with LangChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate,SystemMessagePromptTemplate,\\\n",
        " HumanMessagePromptTemplate\n",
        "\n",
        "system_template = \"\"\"You are a sentiment analysis assistant.\n",
        "                  Classify text as Positive, Negative, or Neutral.\"\"\"\n",
        "\n",
        "user_template = \"\"\"Classify the following sentence:\n",
        "\n",
        "                  {text}\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(user_template)\n",
        "])\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0, api_key=openai.api_key)\n",
        "chain = chat_prompt |llm\n",
        "\n",
        "text_input = \"The product is good, i dont mind buying again.\"\n",
        "\n",
        "result = chain.invoke({\"text\": text_input})\n",
        "print(\"Classification:\", result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_9uibvefhCT",
        "outputId": "833a450d-8ee7-41f8-8d80-f107c860718f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Few shot prompting with LangChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate,SystemMessagePromptTemplate,\\\n",
        " HumanMessagePromptTemplate\n",
        "\n",
        "examples = \"\"\"Text: The service was very slow.\n",
        "              Rating: 1\n",
        "\n",
        "              Text: The steak was awfully good!.\n",
        "              Rating: 5\n",
        "\n",
        "              Text: It was ok, no massive compliants.\n",
        "              Rating: 3\"\"\"\n",
        "\n",
        "system_template = \"\"\"You are a helpful assistant that classifies\n",
        "                      text into ratings between 1 and 5 with\n",
        "                      lowest is 1 and highest is 5\"\"\"\n",
        "\n",
        "\n",
        "user_template = examples+ \"\"\"\n",
        "\n",
        "                Text:{text}\n",
        "                Rating:\n",
        "                \"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(user_template)\n",
        "])\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4.1\", temperature=0.3,\n",
        "                 model_kwargs={\"top_p\": 0.4},\n",
        "                 api_key=openai.api_key)\n",
        "\n",
        "chain = chat_prompt | llm\n",
        "\n",
        "text_input = \"Meal was decent, but I've had better\"\n",
        "\n",
        "result = chain.invoke({\"text\": text_input})\n",
        "\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwym_zMmSQ_m",
        "outputId": "c9b60aea-2987-4e6b-95b2-94d0b8cc603b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rating: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-step prompting with LangChain\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate,SystemMessagePromptTemplate,\\\n",
        " HumanMessagePromptTemplate\n",
        "\n",
        "system_template = \"\"\"You are a blog writer who writes in a pragmatic style\n",
        "                      with no more than 100 words. Output your key summary in\n",
        "                      no more than 5 bullet points \"\"\"\n",
        "\n",
        "user_template = \"\"\"Compose a travel blog on {text} as follows:\n",
        "Step 1: Introduce the destination\n",
        "Step 2: Share personal adventures during the trip\n",
        "Step 3: Summarize the journey \"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(user_template)\n",
        "])\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4.1\", temperature=0.3,\n",
        "                 model_kwargs={\"top_p\": 0.4},\n",
        "                 api_key=openai.api_key)\n",
        "\n",
        "chain = chat_prompt | llm\n",
        "\n",
        "text_input = \"Goa\"\n",
        "\n",
        "result = chain.invoke({\"text\": text_input})\n",
        "\n",
        "print(result.content)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnrpyDoMfhFc",
        "outputId": "58f0e6e9-5980-4f6b-b516-d004b4ffc224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Goa, India’s beach paradise, is a vibrant blend of sun, sand, and culture. From lively markets to serene churches, it’s a destination that caters to every traveler.\n",
            "\n",
            "During my trip, I soaked up the sun at Palolem Beach, explored the historic forts of Aguada and Chapora, and sampled spicy Goan curries at a beach shack. A sunset cruise on the Mandovi River was a highlight, offering stunning views and local music.\n",
            "\n",
            "Goa’s laid-back vibe, friendly locals, and diverse experiences make it a must-visit. I left with unforgettable memories and a longing to return.\n",
            "\n",
            "**Key Summary:**\n",
            "- Goa offers beautiful beaches and rich culture.\n",
            "- Explored forts, beaches, and local cuisine.\n",
            "- Enjoyed a sunset cruise on the Mandovi River.\n",
            "- Friendly locals and relaxed atmosphere.\n",
            "- Left with lasting memories and a desire to revisit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain-of-Thought\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate,SystemMessagePromptTemplate,\\\n",
        " HumanMessagePromptTemplate\n",
        "\n",
        "system_template = \"\"\"You are a mathematics arithmetics specialist.\n",
        "                      Your objective is to solve problems \"\"\"\n",
        "\n",
        "user_template = \"\"\" {text}\n",
        "A: Let's think step by step\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(user_template)\n",
        "])\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4.1\", temperature=0.3,\n",
        "                 model_kwargs={\"top_p\": 0.4},\n",
        "                 api_key=openai.api_key)\n",
        "\n",
        "chain = chat_prompt | llm\n",
        "\n",
        "text_input = \"\"\"Q: You start with 15 books in your collection. At the bookstore,\n",
        "you purchase 8 new books. Then, you lend 3 to your friend and 2 to your cousin.\n",
        "Later, you visit another bookstore and buy 5 more books. How many books do you\n",
        "have now? \"\"\"\n",
        "\n",
        "result = chain.invoke({\"text\": text_input})\n",
        "\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m-QJRj-fhIU",
        "outputId": "ca240ac8-e50c-40f6-b6db-4802cdb257af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Let's solve the problem step by step:\n",
            "\n",
            "1. **Start with 15 books.**\n",
            "2. **Purchase 8 new books:**  \n",
            "   15 + 8 = 23 books\n",
            "3. **Lend 3 to your friend:**  \n",
            "   23 - 3 = 20 books\n",
            "4. **Lend 2 to your cousin:**  \n",
            "   20 - 2 = 18 books\n",
            "5. **Buy 5 more books:**  \n",
            "   18 + 5 = 23 books\n",
            "\n",
            "**Final Answer:**  \n",
            "You have **23 books** now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Self-consistency\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate,SystemMessagePromptTemplate,\\\n",
        " HumanMessagePromptTemplate\n",
        "\n",
        "system_template = \"\"\"You are a mathematics arithmetics specialist.\n",
        "                      Your objective is to solve problems \"\"\"\n",
        "\n",
        "self_consistency_instruction = \"\"\"Imagine three completely independent experts\n",
        "who reason differently are answering the question. The final answer is obtained\n",
        "by majority vote.\"\"\"\n",
        "\n",
        "user_template = self_consistency_instruction + \"\"\"\\n The question is: {text}\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(user_template)\n",
        "])\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4.1\", temperature=0.2,\n",
        "                 model_kwargs={\"top_p\": 0.3},\n",
        "                 api_key=openai.api_key)\n",
        "\n",
        "chain = chat_prompt | llm\n",
        "\n",
        "text_input = \"\"\"If there are 10 cars in the parking lot and 3 more cars arrive.\n",
        "Half the original number of cars leave. Then, half of the current number of cars\n",
        "arrive. How many cars are there in the parking?\"\"\"\n",
        "\n",
        "result = chain.invoke({\"text\": text_input})\n",
        "\n",
        "print(result.content)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkxOjhfnfhLN",
        "outputId": "58156bf8-d894-4578-bad2-fbe79ac9f652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let’s have three independent experts reason through the problem:\n",
            "\n",
            "---\n",
            "\n",
            "**Expert 1: Step-by-step calculation**\n",
            "\n",
            "1. **Start:** 10 cars in the parking lot.\n",
            "2. **3 more arrive:** 10 + 3 = 13 cars.\n",
            "3. **Half the original number (10) leave:** 10 / 2 = 5 leave. 13 - 5 = 8 cars remain.\n",
            "4. **Half of the current number (8) arrive:** 8 / 2 = 4 arrive. 8 + 4 = **12 cars**.\n",
            "\n",
            "---\n",
            "\n",
            "**Expert 2: Logical breakdown**\n",
            "\n",
            "- Initial: 10 cars.\n",
            "- 3 arrive: 13.\n",
            "- 5 (half of 10) leave: 13 - 5 = 8.\n",
            "- 4 (half of 8) arrive: 8 + 4 = **12 cars**.\n",
            "\n",
            "---\n",
            "\n",
            "**Expert 3: Algebraic approach**\n",
            "\n",
            "Let’s use variables:\n",
            "- Start: \\( x = 10 \\)\n",
            "- After 3 arrive: \\( x + 3 = 13 \\)\n",
            "- After half original leave: \\( 13 - (10/2) = 13 - 5 = 8 \\)\n",
            "- After half current arrive: \\( 8 + (8/2) = 8 + 4 = 12 \\)\n",
            "\n",
            "---\n",
            "\n",
            "**Majority vote:**  \n",
            "All three experts agree.\n",
            "\n",
            "**Final answer:**  \n",
            "\\[\n",
            "\\boxed{12}\n",
            "\\]  \n",
            "There are **12 cars** in the parking lot.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tree-Of-Thought\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.llm import LLMChain\n",
        "\n",
        "# Solutions Chain\n",
        "solutions_template = \"\"\"\n",
        "Generate {num_solutions} distinct solutions for {problem}. Consider\n",
        "factors like {factors}.\n",
        "\n",
        "Solutions:\n",
        "\"\"\"\n",
        "\n",
        "solutions_prompt = PromptTemplate(\n",
        "    template=solutions_template,\n",
        "    input_variables=[\"problem\",\"factors\",\"num_solutions\"]\n",
        "    )\n",
        "\n",
        "llm_1 = ChatOpenAI(model_name=\"gpt-4.1\", temperature=0.2,\n",
        "                 model_kwargs={\"top_p\": 0.3},\n",
        "                 api_key=openai.api_key)\n",
        "\n",
        "solutions_chain = LLMChain(llm=llm_1,\n",
        "                           prompt=solutions_prompt,\n",
        "                           output_key=\"solutions\")\n",
        "\n",
        "# Evaluation Chain\n",
        "evaluation_template = \"\"\"\n",
        "Evaluate each solution in {solutions} by analyzing pros, cons, feasibility, and\n",
        "probability of success.\n",
        "\n",
        "Evaluations:\n",
        "\"\"\"\n",
        "\n",
        "evaluation_prompt = PromptTemplate(\n",
        "    template=evaluation_template,\n",
        "    input_variables=[\"solutions\"]\n",
        ")\n",
        "\n",
        "llm_2 = ChatOpenAI(model_name=\"gpt-4.1\", temperature=0.2,\n",
        "                 model_kwargs={\"top_p\": 0.3},\n",
        "                 api_key=openai.api_key)\n",
        "\n",
        "evaluation_chain = LLMChain(llm=llm_2,\n",
        "                            prompt=evaluation_prompt,\n",
        "                            output_key=\"evaluations\")\n",
        "\n",
        "\n",
        "# Reasoning Chain\n",
        "reasoning_template = \"\"\"\n",
        "For the most promising solutions in {evaluations}, explain scenarios,\n",
        "implementation strategies, partneships needed, and handling potential obstacles.\n",
        "\n",
        "Enhanced Resoning:\n",
        "\"\"\"\n",
        "\n",
        "reasoning_prompt = PromptTemplate(\n",
        "    template=reasoning_template,\n",
        "    input_variables=[\"evaluations\"]\n",
        ")\n",
        "\n",
        "llm_3 = ChatOpenAI(model_name=\"gpt-4.1\", temperature=0.2,\n",
        "                 model_kwargs={\"top_p\": 0.3},\n",
        "                 api_key=openai.api_key)\n",
        "\n",
        "reasoning_chain = LLMChain(llm= llm_3,\n",
        "                           prompt= reasoning_prompt,\n",
        "                           output_key=\"enhanced_reasoning\")\n",
        "\n",
        "#Ranking Chain\n",
        "\n",
        "ranking_template = \"\"\"\n",
        "Based on the evaluations and reasoning, rank the solutions in\n",
        "{enhanced_reasoning} from most to least promising.\n",
        "\n",
        "Ranked Solutions:\n",
        "\"\"\"\n",
        "ranking_prompt = PromptTemplate(\n",
        "    template=ranking_template,\n",
        "    input_variables=[\"enhanced_reasoning\"]\n",
        ")\n",
        "\n",
        "llm_4 = ChatOpenAI(model_name=\"gpt-4.1\", temperature=0.2,\n",
        "                 model_kwargs={\"top_p\": 0.3},\n",
        "                 api_key=openai.api_key)\n",
        "\n",
        "\n",
        "ranking_chain = LLMChain(llm= llm_4,\n",
        "                           prompt= ranking_prompt,\n",
        "                           output_key=\"ranked_solutions\")\n",
        "\n",
        "# Adding all chains in sequence\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "total_chain = SequentialChain(\n",
        "    chains = [solutions_chain, evaluation_chain, reasoning_chain,\n",
        "              ranking_chain],\n",
        "    input_variables = [\"problem\",\"factors\",\"num_solutions\"],\n",
        "    output_variables= [\"ranked_solutions\"]\n",
        ")\n",
        "\n",
        "print(total_chain.run(\n",
        "    problem = \"Purchase Car in Bangalore\",\n",
        "    factors = \"\"\"Requirements for high power, low maintenance, and moderate\n",
        "    mileage\"\"\",\n",
        "    num_solutions=3\n",
        "))\n"
      ],
      "metadata": {
        "id": "VLI9RVBHfhOW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c42895a-fb78-4470-9a49-ad8ec29a37fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Certainly! Based on the detailed evaluations, scenarios, implementation strategies, partnerships, and ways to handle obstacles, here is the **ranked list of solutions from most to least promising for a car purchase in Bangalore**:\n",
            "\n",
            "---\n",
            "\n",
            "## **1. Hyundai Creta 1.5L Petrol (Compact SUV)**\n",
            "**Most Promising**\n",
            "\n",
            "- **Why?**  \n",
            "  - Best suited for Bangalore’s mixed and often poor road conditions due to higher ground clearance and robust build.\n",
            "  - Offers modern features, safety tech, and comfort for families.\n",
            "  - Versatile for both city and highway use.\n",
            "  - High resale value and strong after-sales network.\n",
            "- **Obstacles Handled:**  \n",
            "  - Early booking and dealer negotiation can mitigate long waiting periods.\n",
            "  - Parking aids and driving practice address size/parking concerns.\n",
            "  - Maintenance packages help with running costs.\n",
            "\n",
            "---\n",
            "\n",
            "## **2. Honda City 1.5L VTEC (Petrol Sedan)**\n",
            "**Very Promising**\n",
            "\n",
            "- **Why?**  \n",
            "  - Excellent for those prioritizing comfort, driving pleasure, and long-term reliability.\n",
            "  - Ideal for city and highway use, especially if ground clearance is manageable in your routes.\n",
            "  - Strong brand reputation and resale value.\n",
            "- **Obstacles Handled:**  \n",
            "  - Underbody protection and careful driving can address ground clearance.\n",
            "  - Negotiation and pre-owned options can help with higher upfront cost.\n",
            "  - Top-rated service centers ensure good after-sales experience.\n",
            "\n",
            "---\n",
            "\n",
            "## **3. Maruti Suzuki Baleno 1.2L Petrol (Premium Hatchback)**\n",
            "**Practical, but Less Versatile**\n",
            "\n",
            "- **Why?**  \n",
            "  - Best for budget-conscious buyers focused on city driving, low running costs, and easy maintenance.\n",
            "  - Excellent resale value and widespread service network.\n",
            "  - Compact size is perfect for Bangalore’s traffic and parking.\n",
            "- **Obstacles Handled:**  \n",
            "  - Test drives and variant selection can address power/build concerns.\n",
            "  - Extended warranty and careful driving improve long-term experience.\n",
            "  - Aftermarket accessories can fill feature gaps.\n",
            "\n",
            "---\n",
            "\n",
            "### **Summary Table (Ranked)**\n",
            "\n",
            "| Rank | Solution         | Best For                      | Key Strengths                        | Main Obstacles           | Handling Strategies                  |\n",
            "|------|------------------|------------------------------|--------------------------------------|--------------------------|--------------------------------------|\n",
            "| 1    | Hyundai Creta    | Bad roads, features, safety  | Versatility, comfort, tech, resale   | Price, parking, mileage  | Early booking, parking aids          |\n",
            "| 2    | Honda City       | Power, comfort, reliability  | Driving pleasure, reliability, space | Ground clearance, price  | Underbody protection, negotiate deals|\n",
            "| 3    | Maruti Baleno    | City use, low cost, resale   | Economy, ease of use, maintenance    | Power, build, features   | Test drive, extended warranty        |\n",
            "\n",
            "---\n",
            "\n",
            "## **Conclusion & Recommendation\n",
            "\n",
            "1. **Hyundai Creta** is the most promising overall for Bangalore, especially if you want an all-rounder for city and rough roads.\n",
            "2. **Honda City** is a close second, excelling in comfort and reliability if ground clearance is not a major issue for your routes.\n",
            "3. **Maruti Baleno** is the most practical for pure city use and budget-friendliness, but less versatile for rough roads or highway comfort.\n",
            "\n",
            "**Next Steps:**  \n",
            "- Test drive all three.\n",
            "- Compare offers, waiting periods, and after-sales support.\n",
            "- Choose based on your top priorities and the scenarios that best match your needs.\n",
            "\n",
            "---\n",
            "\n",
            "**This ranking maximizes your chances of a satisfying, future-proof car purchase in Bangalore.**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GpGFUEzK8Url"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZYTzGB_I8Uut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k5obLbex8UyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kXVvaIEu8U1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hiHeduCifhRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8LR_O1_8fhU0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}